{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d402f4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning with Keras and Tensorflow\n",
    "*Stats 507, Fall 2021*\n",
    "\n",
    "James Henderson, PhD  \n",
    "December 2, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f123620c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    " - [Neural Networks](#/slide-2-0)\n",
    " - [Tensorflow](#/slide-3-0)\n",
    " - [Keras](#/slide-4-0)\n",
    " - [Keras Pipeline](#/slide-5-0)\n",
    " - [Regularization](#/slide-6-0)\n",
    " - [Takeaways](#/slide-7-0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac5793",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Demo\n",
    " - Read these slides alongside the demo `keras_demo.py` from the course repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e746e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks\n",
    " - Neural networks models related features (inputs) to targets (outputs) using\n",
    "   layers of activation units (\"neurons\").\n",
    " - The dimensions of the target(s) and features determine the size of the input\n",
    "   and output layers.\n",
    " - The \"hidden\" layers in between are determined by the modeler.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a3ca6",
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deep Learning\n",
    " - *Deep Learning* refers to learning with neural network models containing \n",
    "   multiple hidden layers. \n",
    " - Works well for *representational* data (e.g. images) as layers allow for\n",
    "   \"learning\" abstractions that can lead to good regression and classification\n",
    "   performance. \n",
    " - *Deep neural networks* are universal function approximators. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae21c67",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activation Units  \n",
    " - Activation units in one layer are functions of the units in a previous layer\n",
    "   (or layers). \n",
    " - Each activation unit represents a nonlinear transform (\"activation function\")\n",
    "   of a linear function of other units.\n",
    " - This linear function is determined by a vector of *weights* and a *bias* \n",
    "   (intercept) term.\n",
    " - Here is a model $f$ with two hidden layers relating $x \\mapsto y$:\n",
    "\n",
    "   $$\n",
    "   f(x) = g_2(W_2g_1(W_1x + b_1) + b_2). \n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e176f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training \n",
    "- The *weights* and *biases* represent the parameters to be trained using \n",
    "  the training data. \n",
    "- Training is done to minimize the training loss, based on a loss function\n",
    "  measuring how well/poorly the models outputs (predictions) match the training\n",
    "  labels. \n",
    "- This is done by using the gradient of the loss function with respect to these\n",
    "  parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb263e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training \n",
    "- Parameters (weights and biases) are optimized using the gradient and some\n",
    "  variation of *stochastic gradient descent*.\n",
    "- *Stochastic* optimization helps to avoid local minima. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7dd93",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimizers \n",
    "- Because neural networks can be complicated, the gradient - computed through\n",
    "  back-propagation (chain rule) - often *vanishes* or *explodes*. \n",
    "- This is generally solved by adapting the learning rate or using *momentum* -\n",
    "  smoothing the gradient at each step using a local/running average.\n",
    "- The \"RMSprop\" and \"ADAM\" optimizers automatically adapt the learning rate and \n",
    "  tend to be the best default options.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d99978",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batch size\n",
    "- A *batch* is a subset of the data used for computing a gradient update. \n",
    "- An *epoch* is how many training steps are needed to use each sample once,\n",
    "  based on the batch size.\n",
    "- Smaller batch size leads to slower learning rate and longer computation time.\n",
    "- Larger batch size leads to larger learning rate and shorter computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb038f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TensorFlow\n",
    " - TensorFlow is an \"end-to-end ML platform\" developed at Google.  \n",
    " - It helps to think of TensorFlow as the \"backend\" engine for deep learning\n",
    "   with Keras. \n",
    " - Efficient and highly scalable tensor (array) operations on CPU, GPU, and TPU. \n",
    " - Automatically computes gradients - enabling quick training of arbitrary\n",
    "   differentiable models. \n",
    " - (You should be using TensorFlow 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5a22b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras\n",
    " - Keras is the (official) high-level interface (API) for TensorFlow. \n",
    " - Keras [aims][ka] to be: *simple*, *flexible*, and *powerful*. \n",
    " - Using Keras allows you to quickly specify, train, and evaluate deep learning\n",
    "   models.  \n",
    " [ka]: keras.io/about/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899e41e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras APIs\n",
    " - There are three ways to build DNN models with Keras:\n",
    "   - Using the `Sequential` model class, \n",
    "   - Using the functional API,\n",
    "   - Defining a sub-class inheriting from the model class.\n",
    " - The `Sequential` model class is simplest, but limited to a linear\n",
    "   topology for layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae21a06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras in 5 steps: 0\n",
    "- Prepare the data in the TensorFlow format, shuffle, and batch after\n",
    "  range normalizing. \n",
    "- To read from a pandas DataFrame use `tf.data.Dataset.from_tensor_slices()`.\n",
    "- Use the dataset's `.shuffle()` and `.batch()` methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4dde04",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras in 5 steps: 1\n",
    "- Define your model topology. \n",
    "- Use `Sequential()` and the resulting model's `.add()` method to add layers.\n",
    "- Or pass a list of layers to `Sequential()`. \n",
    "- `Sequential()` is in `tf.keras.models`.\n",
    "- Layers are defined in `tf.keras.layers`.\n",
    "- Input layer can be inferred, output layer and activation should match target \n",
    "  and loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb48b1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras in 5 steps: 2\n",
    "- Compile the model using its `.compile()` method. \n",
    "- Choose an optimizer and loss function.\n",
    "- Can add *metrics* for evaluation in addition to the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61189041",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras in 5 steps: 3\n",
    "- Call the compiled model's `.fit()` method with the training and fit methods.\n",
    "- Choose a number of *epochs* -- passes over the entire dataset for gradient\n",
    "  updates. \n",
    "- Can define a batch-size here as well.\n",
    "- Can pass validation data for epoch-by-epoch hold out evaluation. \n",
    "- Can also define *callbacks* here, e.g. for early stopping. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c8416",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras in 5 steps: 4\n",
    "- Evaluate using training history or use the `.evaluate()` and/or `.predict()`\n",
    "  methods. \n",
    "- Modify model and repeat steps 1-4 as needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b3cbc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    " - There are several ways to regularize a neural network model:\n",
    "   + Reduce the number of parameters by using smaller or fewer layers,\n",
    "   + Impose $L1$ or $L2$ penalties on weights from one or more layers,\n",
    "   + Using \"dropout\" to encourage redundancy of key abstractions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2460f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regularizing weights  \n",
    " - Implement $L1$ or $L2$ regularization for a layer using the\n",
    "   `kernel_regularizer` argument and an `l1()` or `l2()` instance from\n",
    "   `tf.keras.regularizers`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc6a2f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout\n",
    "  - Dropout is a regularization technique in which a fraction of weights\n",
    "    are randomly selected to be fixed at 0 (\"dropped out\") for each training\n",
    "    iteration.\n",
    "  - This causes other weights to \"compensate\" and can reduce over-fitting. \n",
    "  - `Dropout()` can be added as a layer affecting the previous `Dense()` layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d156d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Takeaways\n",
    " - The Keras API is a powerful high-level interface to the TensorFlow library\n",
    "   for specifying, training, and evaluating neural network models.\n",
    " - Use the `Sequential()` model class for models with a linear topology of \n",
    "   layers, the functional API for more complex topologies.\n",
    " - Build model, compile, train, evaluate, adjust and repeat. \n",
    " - Increase the number or sizes of layers or train longer to reduce under\n",
    "   fitting.\n",
    " - Use regularization in the form of penalized weights, dropout layers, or\n",
    "   early stopping to reduce over fitting."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "notebook_metadata_filter": "rise"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "header": "<a href=\"#/slide-0-0\"> <h3> Stats 507 - Deep Learning </a>",
   "progress": true,
   "scroll": true,
   "theme": "solarized",
   "transition": "convex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
