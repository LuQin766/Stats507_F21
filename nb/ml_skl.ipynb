{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7926c62a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning and Sci-Kit Learn\n",
    "*Stats 507, Fall 2021*\n",
    "\n",
    "James Henderson, PhD  \n",
    "November 4 & 9, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2adc4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    " - [Machine Learning](#/slide-2-0)\n",
    " - [Scikit-learn](#/slide-3-0)\n",
    " - [Isolet Demo](#/slide-4-0)\n",
    " - [Training, Validation, and Testing](#/slide-5-0)\n",
    " - [Cross-Validation](#/slide-6-0)\n",
    " - [SVD](#/slide-7-0)\n",
    " - [Regularization](#/slide-8-0)\n",
    " - [Takeaways](#/slide-9-0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ccb8f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning\n",
    " + The Wikipedia entry on [Machine Learning][ml] begins ...\n",
    "\n",
    "> Machine learning (ML) is the study of computer algorithms that can improve\n",
    "> automatically through experience and by the use of data ...\n",
    "> Machine learning algorithms build a model based on sample data, \n",
    "> known as \"training data\", in order to make predictions or decisions without \n",
    "> being explicitly programmed to do so.\n",
    "\n",
    "[ml]: https://en.wikipedia.org/wiki/Machine_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ec124",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ML Domains\n",
    "\n",
    "+ *Supervised learning* uses labeled data and is akin to regression methods\n",
    "    in statistics in that one (or more) variables are treated as dependent.\n",
    "\n",
    "    - Regression - continuous (or at least *interval-valued*) labels,\n",
    "    - Classification - discrete/categorical (often binary) labels.\n",
    "\n",
    "+ *Unsupervised learning* includes clustering, visualization, and\n",
    "   distance-based methods. Seeks to understand structure rather than use\n",
    "   some variables to predict others.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de8589",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Machine Learning\n",
    "> A subset of machine learning is closely related to computational statistics,\n",
    "> which focuses on making predictions using computers; \n",
    "> but not all machine learning is statistical learning. \n",
    "\n",
    "+ All or most of what we cover in this class can be thought of as \n",
    "  *statistical learning*. \n",
    "\n",
    "+ If you want to know more, I highly recommend reading and referring to \n",
    "  *The Elements of Statistical Learning*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae52e60",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ML vs Statistics\n",
    "  - There is no clear boundary between ML and statistics.\n",
    "  - ML tends to focus more on prediction and less on inference.\n",
    "  - ML uses hold-out data rather than sampling-theory for evaluation. \n",
    "  - As a practical matter, I like to make the following distinction:\n",
    "  \n",
    "  > If you evaluate your model using hold out data you're doing ML. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8bfc7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit-learn\n",
    "  - Many ML algorithms and models are available in [scikit-learn][skl] (SKL).\n",
    "  - We'll cover a very small fraction of what's available:\n",
    "     + regularized regression: ridge, lasso, and elastic-net,\n",
    "     + random forests,\n",
    "     + gradient-boosted trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511dc3f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scikit-learn\n",
    "  - I'll be using `sklearn` version 1.0.1.\n",
    "  - I'll typically import individual estimators and functions from `sklearn`.\n",
    "  - Most of what I'll use comes from the `linear_model` and `ensemble` APIs.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fdb039",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Isolet Demo\n",
    "- These slides are without examples and are intended to be used\n",
    "  alongside the Isolet demo from the course repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9495c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training, Validation, and Testing\n",
    "  - In supervised ML, we evaluate our models based on their ability to \n",
    "    make predictions on new cases. \n",
    "  - *Test data* is data set aside to evaluate our (final) model(s). \n",
    "  - *Training* data is used to learn model parameters (to *train* our model).\n",
    "  - A *validation* dataset is data set aside to compare different models fit\n",
    "    to the training data and for tuning *hyper-parameters*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df50a1c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Over-fitting\n",
    "- We use*out-of-sample data* for validation and testing to get unbiased\n",
    "  estimates of the out of sample error.\n",
    "- This helps us avoid *over-fitting*, which occurs when our model fits the\n",
    "  training data well but does so in a way that doesn't *generalize* to new\n",
    "  data.\n",
    "- Over-fitting is an instance of a *bias-variance* tradeoff. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312bcd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation \n",
    " - *Cross validation* is often used in place of a designated validation dataset.\n",
    " - *Cross-validation* makes efficient use of available non-test data by\n",
    "    repeatedly interchanging which observations are considered training and\n",
    "    which validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3949398d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-Validation \n",
    " - *Cross-validation* is (typically) done by dividing the data into sub-groups \n",
    "   called *folds*.\n",
    " - If we have *k* of these groups we refer to it as *k-fold* cross-validation.\n",
    " - The special case when *k* equals the total number of not-test samples is \n",
    "    known as _leave-one-out cross-validation_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d60841",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-Validation \n",
    " - When dividing data into folds it is important that observations be \n",
    "   randomly distributed among the groups. \n",
    " - For example, if you had previously sorted your data set you would not want \n",
    "   to assign folds using adjacent rows. \n",
    " - You can avoid this by randomly shuffling the rows (cases). \n",
    " - This is also true when generating the test-train split. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d82db",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-Validation \n",
    " - If you're data are not *iid* additional care is needed for data splitting.\n",
    " - If you have *block* structured data, with dependence isolated within blocks,\n",
    "   and distinct blocks independent, you generally want keep block together\n",
    "   across folds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e49cc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Singular Value Decomposition\n",
    " - The [singular value decomposition][svd] or SVD is a generalized version\n",
    "   of the Eigen decomposition.  \n",
    " - The SVD breaks a matrix $X$ into three parts - two orthonormal matrices \n",
    "   $U$ and $V$ and a diagonal matrix $D$: $X = UDV'$. \n",
    " - By convention $U$, $D$, and $V$ are ordered so that the diagonal of $D$\n",
    "   is largest in the upper left and smallest in the lower right.  \n",
    " - The values of$D$ are called _singular values_, the columns of $U$ are called \n",
    "   _left singular vectors_ and the columns of $V$ _right singular vectors_. \n",
    "\n",
    "[svd]: https://en.wikipedia.org/wiki/Singular-value_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017fb0e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Principle Components Analysis (PCA)\n",
    " - The *Eigen* decomposition of $X'X$ is, $X'X = \\Gamma \\Lambda \\Gamma'$ \n",
    "   where $\\Gamma'\\Gamma = I$ and $\\Lambda$ is diagonal.\n",
    " - If the columns of $X$ have mean zero, then $X'X$ is proportional to the\n",
    "   sample covariance of $X$.   \n",
    " - The columns of $\\Gamma$ are called *eigenvectors* and the entries along the\n",
    "   diagonal of $\\Lambda$ are the corresponding *eigenvalues*. \n",
    " - The columns of $X\\Gamma$ are the *principle components* of $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda711ce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SVD & PCA\n",
    " - PCA: $X'X = \\Gamma \\Lambda \\Gamma'$ \n",
    " - SVD: $X = UDV'$\n",
    " - $X'X = (VD'U')(UDV') = VD^2V$ \n",
    " - Therefore $V = \\Gamma$ and $D^2 = \\Lambda$. \n",
    " - $XV = UD$ are the principle components of $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e0e8e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SVD in NumPy\n",
    "  - `np.linalg.svd()`\n",
    "  - Returns a tuple `(u, d, vh)` - the `h` is for Hermitian \n",
    "  - The Hermitian is just the transpose for a real matrix.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f06c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    " - Regularization is used to limit over-fitting and to make models \n",
    "   identifiable. \n",
    " - One way to regularize, is to limit the number of singular-vectors \n",
    "   (or other features) used in your model.\n",
    " - Another common way to achieve regularization is to penalize the loss\n",
    "   function used to measure fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed62ec0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ridge Regression \n",
    "- In ridge regression - or $L_2$ regularization - we penalize the loss using\n",
    "  the sum of the squared coefficients (the squared $L_2$ norm). \n",
    "- Logistic regression with a ridge penalty has the following *objective* \n",
    "  function (with $g$ the logistic function):\n",
    "\n",
    "  $$\n",
    "  \\mathscr{L}(b) = \\sum_{i=1^n} -y_i \\log g(x_ib) - (1 - y_i)\\log(1 - g(x_ib)) + \n",
    "    \\lambda \\sum_{k=1}^p b_k^2.\n",
    "  $$ \n",
    "\n",
    "- $\\lambda$ is a *hyper-parameter* that controls the amount of regularization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ba269d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ridge Regression - Tuning  \n",
    "- For logistic regression, we can use the function `LogisticRegressionCV()`\n",
    "  to select the penalty $\\lambda$ for ridge regression.\n",
    "- Use `penalty=\"l2\"` and provide a sequence of regularization parameters `C`.\n",
    "- `C` = $\\frac{1}{\\lambda}$\n",
    "- Defaults to 5-fold CV, pass custom folds to `cv`. \n",
    "- Data are passed to the `.fit()` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa959a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ridge Regression - Tuning  \n",
    "- Key attributes of object returned by `LogisticRegressionCV()` after\n",
    "  calling `.fit()`:\n",
    "   - `.C_` - value of C where loss is minimized,\n",
    "   - `Cs_` - sequence of C's used,\n",
    "   - `.scores_` - *dictionary* of CV scores for each class,\n",
    "   - `.coefs_`, `.intercept_` - model coefficients at best `.C_`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f830adc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ridge Regression - Tuning  \n",
    "- Key methods of object returned by `LogisticRegressionCV()`:\n",
    "   - `.fit(X, y)` - fit the model using training data `X` and `y`, \n",
    "   - `.predict()` - predict class labels,\n",
    "   - `.predict_proba()` - predict probabilities,\n",
    "   - `.score()` - compute the score (loss) on a (new) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a25c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lasso\n",
    "- In the Lasso - or $L_1$ regularized regression - we penalize the loss using\n",
    "  the sum of the absolute value of the coefficients (the $L_1$ norm). \n",
    "- Logistic regression with a Lasso penalty has the following *objective* \n",
    "  function:\n",
    "\n",
    "  $$\n",
    "  \\mathscr{L}(b) = \\sum_{i=1^n} -y_i \\log g(x_ib) - (1 - y_i)\\log(1 - g(x_ib)) + \n",
    "    \\lambda \\sum_{k=1}^p |b_k|. \n",
    "  $$ \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a2a11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lasso - Tuning\n",
    "- Use `LogisticRegressionCV()` and select penalty `l1`. \n",
    "- Also need a compatible solver (e.g. `solver=\"saga\"`). \n",
    "- Otherwise the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d5110",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Elastic-net\n",
    "- The elastic-net interpolates between the $L_1$ and $L_2$ penalties.  \n",
    "\n",
    "  $$\n",
    "  \\mathscr{L}(b) = \\sum_{i=1^n} -y_i \\log g(x_ib) - (1 - y_i)\\log(1 - g(x_ib)) + \n",
    "    \\alpha \\lambda \\sum_{k=1}^p |b_k| +\n",
    "     (1 - \\alpha)\\frac{\\lambda}{2}\\sum_{k=1}^p b_k^2. \n",
    "  $$ \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4017608",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Elastic-net - Tuning\n",
    "- Use `LogisticRegressionCV()` and select penalty `elasticnet` and a\n",
    "  compatible solver.\n",
    "- Control the mixing parameter ($\\alpha$) using `l1_ratio`. \n",
    "- Consider using Ridge to find an appropriate range for `C` first. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01333ae1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Takeaways\n",
    "- Set aside test data to evaluate your ML models.\n",
    "- Use regularization to play the bias-variance tradeoff and avoid \n",
    "  over-fitting.\n",
    "- Use cross validation (or a dedicated validation dataset) to tune \n",
    "  hyper parameters and make model-building decisions. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "notebook_metadata_filter": "rise"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "header": "<a href=\"#/slide-0-0\"> <h3> Stats 507 - ML & sci-kit learn </a>",
   "progress": true,
   "scroll": true,
   "theme": "solarized",
   "transition": "convex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
