{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ea70d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PySpark \n",
    "*Stats 507, Fall 2021*\n",
    "\n",
    "James Henderson, PhD  \n",
    "December 7, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a4446",
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    " - [Hadoop](#/slide-2-0)\n",
    " - [MapReduce](#/slide-3-0)\n",
    " - [HDFS](#/slide-4-0)\n",
    " - [PySpark](#/slide-5-0)\n",
    " - [RDD](#/slide-6-0)\n",
    " - [DataFrame](#/slide-7-0)\n",
    " - [SQL](#/slide-8-0)\n",
    " - [Takeaways](#/slide-9-0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41550925",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hadoop\n",
    "> Apache Hadoop is a collection of open-source software utilities that \n",
    "> facilitates using a network of many computers to solve problems involving\n",
    "> massive amounts of data and computation. It provides a software framework \n",
    "> for distributed storage and processing of big data using the MapReduce \n",
    "> programming model. Hadoop was originally designed for computer clusters built\n",
    "> from commodity hardware, which is still the common use.\n",
    ">  <cite>--[Wikipedia][hadoop] </cite>\n",
    "\n",
    "[hadoop]: https://en.wikipedia.org/wiki/Apache_Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b441979",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hadoop\n",
    " - Software library for *distributed* computing.\n",
    " - Designed to work with consumer-level computers connected over a network.\n",
    " - Intended to be resilient to failures of some cluster components.  \n",
    " - Accomplished through data-replication.\n",
    " - Implements MapReduce paradigm.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e322fda",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hadoop Ecosystem\n",
    " - Map Reduce - framework for distributed computing\n",
    " - HDFS - Hadoop Distributed File System\n",
    " - PySpark - Spark's interactive Python Console. \n",
    " - Yarn - job manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf59fdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce\n",
    " - [MapReduce][mr] is a programming paradigm for working with \"massive\"\n",
    "   data distributed across a Hadoop cluster.\n",
    " - This distributed processing allows programs to flexibly scale to data \n",
    "   measured in petabytes (1 million GB or 1 thousand TB).  \n",
    " \n",
    " [mr]: https://www.ibm.com/topics/mapreduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239adc2a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MapReduce\n",
    " - A MapReduce program consists of a *map* step and a *reduce* step:\n",
    "    + The *map* steps work on chunks of data in parallel and return\n",
    "      key-value pairs.\n",
    "    + The *reduce* step aggregates those pairs into the desired outcome.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fed592",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HDFS\n",
    "- The *Hadoop distributed file system* is a core part of the Hadoop framework.\n",
    "- Splits files into large blocks and distributes them across nodes in a \n",
    "  cluster.\n",
    "- Often used with replication.  Common to use 128 MB blocks with 3x replication.\n",
    "- This is a nice introduction to HDFS using Legos:\n",
    "  https://youtu.be/4Gfl0WuONMY  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91b39c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## HDFS\n",
    " - The *distributed file system* is different from the POSIx home directory\n",
    "   mounted to the login nodes used to access the hadoop cluster. \n",
    " - Use linux-like file system commands after `hdfs dfs` to work with files.\n",
    " - Use `hdfs dfs -put <local_file> <path/new_file>` to put data into HDFS.  \n",
    " - Use `hdfs dfs -get <hdfs_file> <local_file>` to get data from HDFS.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c38f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ssh cavium-thunderx.arc-ts.umich.edu\n",
    "hdfs dfs -ls stats507\n",
    "hdfs dfs -ls /user/jbhender/stats507"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8dcb5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## HDFS-FUSE\n",
    " - Navigate to directory `/hadoop-fuse/user/<email>/` and use linux file system\n",
    "   commands without `hdfs dfs` prefix. \n",
    "\n",
    "```bash\n",
    "cd /hadoop-fuse/\n",
    "ls /user/jbhender/stats507/\n",
    "head -5 /user/jbhender/stats507/rectangles.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c54e01e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PySpark\n",
    " - PySpark is an interactive Python console for Spark.\n",
    " - Start a PySpark session as shown below.\n",
    " - Only `--master yarn` is necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5642e64f",
   "metadata": {},
   "source": [
    " ```bash\n",
    " pyspark --master yarn --queue default --num-executors=8 --executor-memory=1g\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c6514",
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Existing Objects\n",
    " - When you launch PySpark, the following instances will be present:\n",
    "    + `spark` - an instance of a `SparkSession()`,\n",
    "    + `sc` - an instance of a `SparkContext()`,\n",
    "    + `sqlContext` - an instance of `SQLContext()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf96112",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batch Mode\n",
    " - To run in batch mode, use `spark-submit`.\n",
    " - For SQL, add: `--conf spark.hadoop.metastore.catalog.default=hive`\n",
    " - Add the lines below to get to the same starting point as the interactive \n",
    "   shell. \n",
    " - Taken from [here][cao].\n",
    "\n",
    " [cao]: https://github.com/caocscar/workshops/blob/master/pyspark/pyspark.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f49030",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ba134",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RDD\n",
    " - Spark stores data in a *Resilient Distributed Dataset* or [RDD][rdd].\n",
    " - An RDD is immutable, transformations result in a new RDD.\n",
    " - Resiliency is accomplished through data redundancy or *partitions* which\n",
    "   also enables parallelism.\n",
    "\n",
    " [rdd]: https://spark.apache.org/docs/2.2.1/rdd-programming-guide.html#overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9306e36",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RDD\n",
    " - Use `sc.parallelize()` to distribute the data.\n",
    " - Use `.repartition()` or `.coalesce()` to redistribute an existing RDD.\n",
    " - RDDs support two types of operation *transformations* and *actions*. \n",
    " - We mostly won't use RDDs directly, instead using higher level SQL/DataFrame \n",
    "   instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff19552",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DataFrame\n",
    " - In [Spark][df], a \"DataFrame is a *Dataset* organize into named columns.\" \n",
    " - DataFrame instances support distributed processing.\n",
    " - Convert from a pandas DataFrame using `createDataFrame()`.\n",
    " - See more [here][qsdf].\n",
    "\n",
    " [df]: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    " [qsdf]: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10478745",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SQL\n",
    " - DataFrames can be registered as SQL tables using the \n",
    "   `sqlContext.registerDataFrameAsTable()` method.\n",
    " - (Better) use the DataFrame's `.registerTempTable()` method so table's \n",
    "   don't persis across jobs. \n",
    " - Run SQL queries against registered tables using `sqlContext.sql()`. \n",
    " - For all but very simple queries, best to create a string instance \n",
    "   for the query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91849b2b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SQL/DataFrame Results\n",
    " - Use `.show()` to print a DataFrame (e.g. resulting from a SQL query).\n",
    " - Use `.collect()` to gather the results into memory.\n",
    " - By default, PySpark uses *lazy evaluation* -- results are formed only as \n",
    "   needed.\n",
    " - Use `.persist()` to save results so they don't need to be recomputed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b9caf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SQL\n",
    "  - *Structure Query Language* or SQL is a standard syntax for expressing\n",
    "    data frame (\"table\") operations.\n",
    "  - SQL is an *imperative* syntax - you specify what the result should look \n",
    "    like, rather than *declaring how* to achieve it.\n",
    "  - SQL is a common way to interact with RDDs and DataFrames in PySpark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095ee77",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Canonical Order\n",
    " - SQL statements appear using clauses in the canonical order below.\n",
    " - Not all clauses are present in all statement.\n",
    " - Often use LEFT/INNER/FULL OUTER `JOIN`s after `FROM`/`WHERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c03feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "FROM\n",
    "WHERE\n",
    "GROUP BY\n",
    "HAVING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62e90ff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SQL Syntax\n",
    " - Identify the source table in `FROM`.\n",
    " - Use `WHERE` to specify a subset of data to include using conditions on \\\n",
    "   *existing tables* (in `FROM`)\n",
    " - Choose, transform, and rename columns using `SELECT`.\n",
    " - Use [aggregation functions][af] and `GROUP BY` for split-apply-combined \n",
    "   operations.\n",
    "\n",
    "[rf]: https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#aggregate-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa70639",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Aliases\n",
    "- After `FROM` use a short name to alias a table.\n",
    "- Especially useful when table name needs a prefix with joins. \n",
    "- In `SELECT` rename a column/computations using `as`. \n",
    "- Create a table from a query by aliasing the statement with `AS`:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE <name> AS SELECT ...\n",
    "```\n",
    "\n",
    "<!-- #region {\"slideshow\": {\"slide_type\": \"subslide\"}} -->\n",
    "## Anonymous Tables\n",
    " - For complex queries, often helpful to express an intermediate results using\n",
    "   an *anonymous* table `(SELECT ...) a`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69ec61",
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## JOINS\n",
    " - Work with data from multiple table by creating a *join* (a temporary merge).\n",
    " - Specify the keys to merge on using `ON` e.g. `ON a.id = b.id`.\n",
    " - Prefer `LEFT JOIN` or `INNER JOIN` for consistency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ef4f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Takeaways\n",
    " - (Py)Spark enables parallel computations on massive datasets through\n",
    "    distributed computing and data parallelism.\n",
    " - Resiliency is achieved through redundancy. \n",
    " - Not generally the best choice for data that fits in memory. \n",
    " - Basic SQL is essential knowledge beyond Spark."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "notebook_metadata_filter": "rise"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "header": "<a href=\"#/slide-0-0\"> <h3> Stats 507 - PySpark </a>",
   "progress": true,
   "scroll": true,
   "theme": "solarized",
   "transition": "convex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
